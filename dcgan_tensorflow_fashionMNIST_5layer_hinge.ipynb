{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_tensorflow_fashionMNIST_5layer_hinge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wonjerry/DCGAN/blob/master/dcgan_tensorflow_fashionMNIST_5layer_hinge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1AgpktNPMg6",
        "colab_type": "text"
      },
      "source": [
        "# [DCGAN](https://hyeongminlee.github.io/post/gan003_dcgan/)\n",
        "\n",
        "# Hyper-Parameters\n",
        "- Image Size = 32x32\n",
        "- Batch Size = 64 (~32 is OK)\n",
        "- Learning Rate = 0.0002\n",
        "- Adam_beta1 = 0.5\n",
        "- z_dim = 100\n",
        "- Epoch = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE4PmnDtOvLr",
        "colab_type": "code",
        "outputId": "408b87f3-bed5-4a2c-cffa-248f9394a6e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar10\n",
        "from keras.datasets import mnist\n",
        "import PIL\n",
        "import matplotlib\n",
        "\n",
        "print(tf.VERSION)\n",
        "\n",
        "# y_train_은 라벨임\n",
        "(x_train, y_train_), (x_test, y_test_) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "print(x_train.shape)\n",
        "img2 = np.reshape(x_train[0], [28, 28])\n",
        "plt.imshow(img2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "(60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rkS_n_hRjXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "RESULT_DIR = '/content/gdrive/My Drive/results'\n",
        "CKPT_DIR = RESULT_DIR + '/ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2jj0BplQaMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def save_visualization(X, nh_nw, save_path='/content/gdrive/My Drive/results/sample.jpg'):\n",
        "    nh, nw = nh_nw\n",
        "    h, w = X.shape[1], X.shape[2]\n",
        "    img = np.zeros((h * nh, w * nw, 3))\n",
        "\n",
        "    for n, x in enumerate(X):\n",
        "        j = int(n / nw)\n",
        "        i = int(n % nw)\n",
        "        img[j * h:j * h + h, i * w:i * w + w, :] = x\n",
        "\n",
        "    matplotlib.image.imsave(save_path, img)\n",
        "\n",
        "#     scipy.misc.imsave(save_path, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0d1MZ_SPjhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class for batch normalization node\n",
        "class batch_norm(object):\n",
        "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
        "        with tf.variable_scope(name):\n",
        "            self.epsilon = epsilon\n",
        "            self.momentum = momentum\n",
        "            self.name = name\n",
        "\n",
        "    def __call__(self, x, train=True):\n",
        "        return tf.contrib.layers.batch_norm(x,\n",
        "                                            decay=self.momentum,\n",
        "                                            updates_collections=None,\n",
        "                                            epsilon=self.epsilon,\n",
        "                                            scale=True,\n",
        "                                            is_training=train,\n",
        "                                            scope=self.name,\n",
        "                                            reuse=tf.AUTO_REUSE  # if tensorflow vesrion < 1.4, delete this line\n",
        "                                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD0-3k2vaQBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
        "    r\"\"\"Compute focal loss for predictions.\n",
        "        Multi-labels Focal loss formula:\n",
        "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
        "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
        "    Args:\n",
        "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
        "        num_classes] representing the predicted logits for each class\n",
        "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
        "        num_classes] representing one-hot encoded classification targets\n",
        "     weights: A float tensor of shape [batch_size, num_anchors]\n",
        "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
        "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
        "    Returns:\n",
        "        loss: A (scalar) tensor representing the value of the loss function\n",
        "    \"\"\"\n",
        "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
        "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
        "    \n",
        "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
        "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
        "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
        "    \n",
        "    # For negative prediction, only need consider back part loss, front part is 0;\n",
        "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
        "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
        "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
        "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
        "    return tf.reduce_sum(per_entry_cross_ent)\n",
        "  \n",
        "\n",
        "def discriminator_loss(loss_func, real, fake):\n",
        "    real_loss = 0\n",
        "    fake_loss = 0\n",
        "\n",
        "    if loss_func.__contains__('wgan') :\n",
        "        real_loss = -tf.reduce_mean(real)\n",
        "        fake_loss = tf.reduce_mean(fake)\n",
        "\n",
        "    if loss_func == 'lsgan' :\n",
        "        real_loss = tf.reduce_mean(tf.squared_difference(real, 1.0))\n",
        "        fake_loss = tf.reduce_mean(tf.square(fake))\n",
        "\n",
        "    if loss_func == 'gan' or loss_func == 'dragan' :\n",
        "        real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real), logits=real))\n",
        "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake), logits=fake))\n",
        "\n",
        "    if loss_func == 'hinge' :\n",
        "        real_loss = tf.reduce_mean(relu(1.0 - real))\n",
        "        fake_loss = tf.reduce_mean(relu(1.0 + fake))\n",
        "        \n",
        "    if loss_func == 'focal' :\n",
        "        gamma = 2.0\n",
        "        real_loss = -tf.reduce_mean(((1 - real)**gamma) * tf.log(real))\n",
        "        fake_loss = -tf.reduce_mean(((1 - (1 - fake))**gamma) * tf.log(1 - fake))\n",
        "\n",
        "    loss = real_loss + fake_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "def generator_loss(loss_func, fake):\n",
        "    fake_loss = 0\n",
        "\n",
        "    if loss_func.__contains__('wgan') :\n",
        "        fake_loss = -tf.reduce_mean(fake)\n",
        "\n",
        "    if loss_func == 'lsgan' :\n",
        "        fake_loss = tf.reduce_mean(tf.squared_difference(fake, 1.0))\n",
        "\n",
        "    if loss_func == 'gan' or loss_func == 'dragan' :\n",
        "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake), logits=fake))\n",
        "\n",
        "    if loss_func == 'hinge' :\n",
        "        fake_loss = -tf.reduce_mean(fake)\n",
        "        \n",
        "    if loss_func == 'focal' :\n",
        "        gamma = 2.0\n",
        "        fake_loss = -tf.reduce_mean(((1 - (1 - fake))**gamma) * tf.log(1 - fake))\n",
        "\n",
        "    loss = fake_loss\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huiy-aK7PqKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# leaky relu function\n",
        "def lrelu(X, leak=0.2):\n",
        "    f1 = 0.5 * (1 + leak)\n",
        "    f2 = 0.5 * (1 - leak)\n",
        "    return f1 * X + f2 * tf.abs(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiwVr3iaPr6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGAN:\n",
        "    # Network Parameters\n",
        "    def __init__(self, sess, batch_size):\n",
        "        self.learning_rate = 0.0002\n",
        "        self.generator_learning_rate = 1e-3\n",
        "        self.discriminator_learning_rate = 0.0002\n",
        "\n",
        "        self.sess = sess\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.image_shape = [28, 28, 1]\n",
        "\n",
        "        self.dim_z = 100\n",
        "        self.dim_W1 = 1024\n",
        "        self.dim_W2 = 512\n",
        "        self.dim_W3 = 128\n",
        "        self.dim_W4 = 16\n",
        "        self.dim_W5 = 1\n",
        "\n",
        "        self.G_W1 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W1, self.dim_z], stddev=0.02), name=\"G_W1\")\n",
        "        self.G_bn1 = batch_norm(name=\"G_bn1\")\n",
        "\n",
        "        self.G_W2 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W2, self.dim_W1], stddev=0.02), name='G_W2')\n",
        "        self.G_bn2 = batch_norm(name=\"G_bn2\")\n",
        "\n",
        "        self.G_W3 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W3, self.dim_W2], stddev=0.02), name='G_W3')\n",
        "        self.G_bn3 = batch_norm(name=\"G_bn3\")\n",
        "        \n",
        "        self.G_W4 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W4, self.dim_W3], stddev=0.02), name='G_W3')\n",
        "        self.G_bn4 = batch_norm(name=\"G_bn4\")\n",
        "\n",
        "        self.G_W5 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W5, self.dim_W4], stddev=0.02), name='G_W4')\n",
        "\n",
        "        self.D_W1 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W5, self.dim_W4], stddev=0.02), name='D_W1')\n",
        "\n",
        "        self.D_W2 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W4, self.dim_W3], stddev=0.02), name='D_W2')\n",
        "        self.D_bn2 = batch_norm(name=\"D_bn2\")\n",
        "\n",
        "        self.D_W3 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W3, self.dim_W2], stddev=0.02), name='D_W3')\n",
        "        self.D_bn3 = batch_norm(name=\"D_bn3\")\n",
        "        \n",
        "        self.D_W4 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W2, self.dim_W1], stddev=0.02), name='D_W4')\n",
        "        self.D_bn4 = batch_norm(name=\"D_bn4\")\n",
        "\n",
        "        self.D_W5 = tf.Variable(tf.truncated_normal([4, 4, self.dim_W1, 1], stddev=0.02), name='D_W5')\n",
        "\n",
        "        self.gen_params = [\n",
        "            self.G_W1,\n",
        "            self.G_W2,\n",
        "            self.G_W3,\n",
        "            self.G_W4,\n",
        "            self.G_W5\n",
        "        ]\n",
        "\n",
        "        self.discrim_params = [\n",
        "            self.D_W1,\n",
        "            self.D_W2,\n",
        "            self.D_W3,\n",
        "            self.D_W4,\n",
        "            self.D_W5\n",
        "        ]\n",
        "        \n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        self._build_model()\n",
        "        \n",
        "    def _build_model(self):\n",
        "        self.Z = tf.placeholder(tf.float32, [self.batch_size, self.dim_z])\n",
        "\n",
        "        self.image_real = tf.placeholder(tf.float32, [self.batch_size] + self.image_shape)\n",
        "        image_gen = self.generate(self.Z)\n",
        "\n",
        "        d_real = self.discriminate(self.image_real)\n",
        "        d_gen = self.discriminate(image_gen)\n",
        "\n",
        "        self.discrim_cost = discriminator_loss('hinge', d_real, d_gen)\n",
        "        self.gen_cost = generator_loss('hinge', d_gen)\n",
        "\n",
        "        self.train_op_discrim = tf.train.AdamOptimizer(self.discriminator_learning_rate, beta1=0.5).minimize(self.discrim_cost, var_list=self.discrim_params)\n",
        "        self.train_op_gen = tf.train.AdamOptimizer(self.generator_learning_rate, beta1=0.5).minimize(self.gen_cost, var_list=self.gen_params)\n",
        "        \n",
        "    def generate(self, Z):\n",
        "        h1 = tf.reshape(Z, [self.batch_size, 1, 1, self.dim_z])\n",
        "        h1 = tf.nn.conv2d_transpose(h1, self.G_W1, output_shape=[self.batch_size, 2, 2, self.dim_W1],\n",
        "                                    strides=[1, 4, 4, 1])\n",
        "        h1 = tf.nn.relu(self.G_bn1(h1))\n",
        "\n",
        "        h2 = tf.nn.conv2d_transpose(h1, self.G_W2, output_shape=[self.batch_size, 4, 4, self.dim_W2],\n",
        "                                    strides=[1, 2, 2, 1])\n",
        "        h2 = tf.nn.relu(self.G_bn2(h2))\n",
        "\n",
        "        h3 = tf.nn.conv2d_transpose(h2, self.G_W3, output_shape=[self.batch_size, 7, 7, self.dim_W3],\n",
        "                                    strides=[1, 2, 2, 1])\n",
        "        h3 = tf.nn.relu(self.G_bn3(h3))\n",
        "        \n",
        "        h4 = tf.nn.conv2d_transpose(h3, self.G_W4, output_shape=[self.batch_size, 14, 14, self.dim_W4],\n",
        "                                    strides=[1, 2, 2, 1])\n",
        "        h4 = tf.nn.relu(self.G_bn4(h4))\n",
        "        \n",
        "        h5 = tf.nn.conv2d_transpose(h4, self.G_W5, output_shape=[self.batch_size, 28, 28, self.dim_W5],\n",
        "                                    strides=[1, 2, 2, 1])\n",
        "        x = tf.nn.tanh(h5)\n",
        "        return x\n",
        "      \n",
        "    def discriminate(self, image):\n",
        "        h1 = lrelu(tf.nn.conv2d(image, self.D_W1, strides=[1, 2, 2, 1], padding='SAME'))\n",
        "        h2 = lrelu(self.D_bn2(tf.nn.conv2d(h1, self.D_W2, strides=[1, 2, 2, 1], padding='SAME')))\n",
        "        h3 = lrelu(self.D_bn3(tf.nn.conv2d(h2, self.D_W3, strides=[1, 2, 2, 1], padding='SAME')))\n",
        "        h4 = lrelu(tf.nn.conv2d(h3, self.D_W4, strides=[1, 2, 2, 1], padding='SAME'))\n",
        "        h5 = lrelu(tf.nn.conv2d(h4, self.D_W5, strides=[1, 4, 4, 1], padding='SAME'))\n",
        "        y = tf.nn.sigmoid(h5)\n",
        "        return y\n",
        "      \n",
        "    def sample_generator(self, noise_z, batch_size=1):\n",
        "        noise_z = np.array(noise_z).reshape([batch_size, self.dim_z])\n",
        "\n",
        "        Z = tf.placeholder(tf.float32, [batch_size, self.dim_z])\n",
        "        h1 = tf.reshape(Z, [batch_size, 1, 1, self.dim_z])\n",
        "        h1 = tf.nn.conv2d_transpose(h1, self.G_W1, output_shape=[batch_size, 2, 2, self.dim_W1],\n",
        "                                    strides=[1, 4, 4, 1])\n",
        "        h1 = tf.nn.relu(self.G_bn1(h1))\n",
        "\n",
        "        output_shape_l2 = [batch_size, 4, 4, self.dim_W2]\n",
        "        h2 = tf.nn.conv2d_transpose(h1, self.G_W2, output_shape=output_shape_l2, strides=[1, 2, 2, 1])\n",
        "        h2 = tf.nn.relu(self.G_bn2(h2))\n",
        "\n",
        "        output_shape_l3 = [batch_size, 7, 7, self.dim_W3]\n",
        "        h3 = tf.nn.conv2d_transpose(h2, self.G_W3, output_shape=output_shape_l3, strides=[1, 2, 2, 1])\n",
        "        h3 = tf.nn.relu(self.G_bn3(h3))\n",
        "\n",
        "        output_shape_l4 = [batch_size, 14, 14, self.dim_W4]\n",
        "        h4 = tf.nn.conv2d_transpose(h3, self.G_W4, output_shape=output_shape_l4, strides=[1, 2, 2, 1])\n",
        "        h4 = tf.nn.relu(self.G_bn4(h4))\n",
        "        \n",
        "        output_shape_l5 = [batch_size, 28, 28, self.dim_W5]\n",
        "        h5 = tf.nn.conv2d_transpose(h4, self.G_W5, output_shape=output_shape_l5, strides=[1, 2, 2, 1])\n",
        "        x = tf.nn.tanh(h5)\n",
        "\n",
        "        generated_samples = self.sess.run(x, feed_dict={Z: noise_z})\n",
        "        generated_samples = (generated_samples + 1.) / 2.\n",
        "        return generated_samples\n",
        "      \n",
        "    def train_gen(self, noise_z):\n",
        "        _, loss_val_G = self.sess.run([self.train_op_gen, self.gen_cost], feed_dict={self.Z: noise_z})\n",
        "        return loss_val_G\n",
        "\n",
        "    def train_discrim(self, batch_xs, noise_z):\n",
        "        _, loss_val_D = self.sess.run([self.train_op_discrim, self.discrim_cost],\n",
        "                                      feed_dict={self.image_real: batch_xs, self.Z: noise_z})\n",
        "        return loss_val_D\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvzqkcdFQcp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(ckpt_name=None):\n",
        "  total_epoch = 1000\n",
        "  batch_size = 256\n",
        "  n_noise = 100\n",
        "\n",
        "  sess = tf.Session()\n",
        "  model = DCGAN(sess, batch_size)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  total_batch = int(x_train.shape[0] / batch_size)\n",
        "  print('total_batch: ', total_batch)\n",
        "\n",
        "  visualization_num = 14 * 14\n",
        "  noise_test = np.random.normal(size=(visualization_num, n_noise))\n",
        "\n",
        "  loss_D = 0.0\n",
        "  loss_G = 0.0\n",
        "  start = 0\n",
        "\n",
        "  ckpt_path = tf.train.latest_checkpoint(CKPT_DIR)\n",
        "  print(ckpt_path)\n",
        "  \n",
        "  if ckpt_path is not None:\n",
        "    model.saver.restore(sess, tf.train.latest_checkpoint(CKPT_DIR))\n",
        "    start = int(ckpt_path[-1]) + 1\n",
        "    print('start: ',  start)\n",
        "\n",
        "  for epoch in range(start, total_epoch):\n",
        "    for step in range(total_batch):\n",
        "      batch_xs = x_train[step * batch_size:(step + 1) * batch_size] # Get the next batch\n",
        "      batch_xs = batch_xs * (2.0 / 255.0) - 1\n",
        "      noise_g = np.random.normal(size=(batch_size, n_noise))\n",
        "      noise_d = np.random.normal(size=(batch_size, n_noise))\n",
        "\n",
        "      # Train Generator twice while epoch % 2 == 0\n",
        "      if epoch % 2 == 0:\n",
        "          adventage = 2\n",
        "      else:\n",
        "          adventage = 1\n",
        "\n",
        "      if step % adventage == 0:\n",
        "          # Train Discriminator and get the loss value\n",
        "          loss_D = model.train_discrim(batch_xs, noise_d)     \n",
        "      # Train Generator and get the loss value\n",
        "      loss_G = model.train_gen(noise_g)\n",
        "\n",
        "      print('Epoch: [', epoch + 1, '/', total_epoch, '], ', 'Step: [', step + 1, '/', total_batch, '], D_loss: ',\n",
        "            loss_D, ', G_loss: ', loss_G)\n",
        "      if step == 0 or (step + 1) % 100 == 0:\n",
        "          generated_samples = model.sample_generator(noise_test, batch_size=visualization_num)\n",
        "          print(generated_samples.shape)\n",
        "          ex_img = np.reshape(generated_samples[0], [28, 28])\n",
        "          plt.imshow(ex_img)\n",
        "          plt.show()\n",
        "          savepath = RESULT_DIR + '/output_' + 'EP' + str(epoch).zfill(3) + \"_Batch\" + str(step + 1).zfill(6) + '.jpg'\n",
        "          save_visualization(generated_samples, (14, 14), save_path=savepath)\n",
        "    \n",
        "    # TODO(wonjerry): make checkpoint name configurable\n",
        "    model.saver.save(sess, (CKPT_DIR + '/model_test'), epoch)  \n",
        "\n",
        "main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}